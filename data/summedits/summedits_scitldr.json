[
  {
    "id": "rJgMlhRctm_og",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "label": 1,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_0",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, sounds, and semantic parsing of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_1",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Idea Generator (NS-IG), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_2",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We introduce the Neuro-Symbolic Concept Learner (NS-CL), a model able to grasp visual concepts, words, and semantic parsing of sentences without explicit guidance on any of them.",
    "label": 1,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_3",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns auditory concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_4",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and syntax parsing of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_5",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Creator (NS-CC), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_6",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic interpretation of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_7",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual representations, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "label": 1,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_8",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Teacher (NS-CT), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_9",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Nurturer (NS-CN), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_10",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Perception Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences with explicit supervision on all of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "antonym_swap",
      "entity_modification",
      "negation_insertion_removal"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_11",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns auditory concepts, phrases, and semantic parsing of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_12",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Generator (NS-CG), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_13",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns auditory concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_14",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Idea Learner (NS-IL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_15",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Trainer (NS-CTR), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_16",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual ideas, words, and semantic parsing of sentences without direct supervision on any of them.",
    "label": 1,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_17",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, phrases, and semantic parsing of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_18",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Evolution Learner (NS-EL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_19",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and syntactic parsing of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_20",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of words without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_21",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences with explicit supervision on all of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_22",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Multi-Symbolic Concept Learner (MS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_23",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without implicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_24",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Educator (NS-CE), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_25",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic identification of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_26",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns auditory concepts, words, and semantic parsing of sentences without implicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_27",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, sentences, and semantic parsing of words without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_28",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Interpretation Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_29",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of paragraphs without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_30",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic interpretation of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJgMlhRctm_31",
    "doc": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "summary": "We propose the Neuro-Symbolic Conceptual Learner (NS-CCL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "label": 0,
    "original_summary": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJIN_4lA-_og",
    "doc": "Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.",
    "summary": "How can we build artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "label": 1,
    "original_summary": "How can we build artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "rJIN_4lA-_0",
    "doc": "Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.",
    "summary": "How can we build artificial agents that worsen social dilemmas (situations where individuals face a temptation to increase their payoffs at zero cost to total welfare)?",
    "label": 0,
    "original_summary": "How can we build artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJIN_4lA-_2",
    "doc": "Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.",
    "summary": "How can we build natural agents that solve social dilemmas (situations where individuals face a temptation to decrease their payoffs at a cost to total welfare)?",
    "label": 0,
    "original_summary": "How can we build artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJIN_4lA-_3",
    "doc": "Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.",
    "summary": "How can we eliminate the need for artificial agents that solve social dilemmas (situations where individuals face a temptation to decrease their payoffs at a cost to total welfare)?",
    "label": 0,
    "original_summary": "How can we build artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJIN_4lA-_4",
    "doc": "Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.",
    "summary": "How can we hinder the construction of artificial agents that solve social dilemmas (situations where individuals face a temptation to decrease their payoffs at a cost to total welfare)?",
    "label": 0,
    "original_summary": "How can we build artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJIN_4lA-_5",
    "doc": "Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.",
    "summary": "How can we build computer systems that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at no cost to total welfare)?",
    "label": 0,
    "original_summary": "How can we build artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJIN_4lA-_9",
    "doc": "Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.",
    "summary": "How can we avoid building artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost of total welfare)?",
    "label": 0,
    "original_summary": "How can we build artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJIN_4lA-_10",
    "doc": "Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.",
    "summary": "How can we construct artificial agents that solve social predicaments (situations where individuals face a temptation to increase their payoffs at a cost to the entire welfare)?",
    "label": 1,
    "original_summary": "How can we build artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "rJIN_4lA-_12",
    "doc": "Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.",
    "summary": "How can we prove experimentally that artificial agents cannot solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "label": 0,
    "original_summary": "How can we build artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "edit_types": [
      "antonym_swap",
      "hallucinated_fact_insertion",
      "negation_insertion_removal"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJIN_4lA-_13",
    "doc": "Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.",
    "summary": "How can we build artificial agents that create bad outcomes in social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "label": 0,
    "original_summary": "How can we build artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "edit_types": [
      "antonym_swap",
      "hallucinated_fact_insertion"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJIN_4lA-_14",
    "doc": "Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.",
    "summary": "How can we prevent the development of artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "label": 0,
    "original_summary": "How can we build artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJIN_4lA-_15",
    "doc": "Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.",
    "summary": "How can we build artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to individual welfare)?",
    "label": 0,
    "original_summary": "How can we build artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJIN_4lA-_16",
    "doc": "Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.",
    "summary": "How can we shape artificial agents that solve social predicaments (situations where individuals face a temptation to elevate their payoffs at a cost to total welfare)?",
    "label": 1,
    "original_summary": "How can we build artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "rJIN_4lA-_18",
    "doc": "Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.",
    "summary": "How can we construct artificial agents that don't solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "label": 0,
    "original_summary": "How can we build artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJIN_4lA-_19",
    "doc": "Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.",
    "summary": "How can we build artificial agents that achieve bad outcomes in social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "label": 0,
    "original_summary": "How can we build artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "edit_types": [
      "antonym_swap",
      "hallucinated_fact_insertion"
    ],
    "split": "evaluation"
  },
  {
    "id": "rJIN_4lA-_20",
    "doc": "Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.",
    "summary": "How can we manufacture artificial agents that deal with social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "label": 1,
    "original_summary": "How can we build artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_og",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 1,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_0",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain machine interface, we show a direct link between attentional effects on motor accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_1",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on temporal accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_2",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain machine interface, we show an indirect link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_3",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive neurostimulator, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_4",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain-machine interface, we show an indirect link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_5",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain machine interface, we show a direct link between visual effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_6",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive EEG-Brain interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_7",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain machine interface, we show no link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification",
      "negation_insertion_removal"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_8",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in fMRI-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_9",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive neurodevice, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_10",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain-computer interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_11",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain-machine interface, we demonstrate a direct link between attentional effects on awareness and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 1,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_12",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain-machine interface, we demonstrate a direct link between attentional effects on perceptual accuracy and neural gain in MEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_13",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain-machine interface, we indicate a direct link between attentional effects on visual accuracy and neural gain in EEG-ASSEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_14",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive neural prosthesis, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_15",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive EEG headset, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_16",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain implant, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_17",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain-machine interface, we observe a direct link between attentional effects on perceptual accuracy and neural loss in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_18",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain machine interface, we fail to show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the monkey brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification",
      "negation_insertion_removal"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_19",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain-machine interface, we indicate a direct link between attention's effects on perceptual accuracy and neural gain in ECG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_21",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive neural interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_22",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive neuroprosthetic, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_23",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain-implantable device, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_24",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain machine interface, we show a direct link between cognitive effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_25",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain machine interface, we show a direct link between attentional effects on perceptual accuracy and neural loss in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_26",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain machine interface, we do not show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification",
      "negation_insertion_removal"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_27",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain-machine interface, we fail to show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "negation_insertion_removal"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_28",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain machine interface, we fail to show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification",
      "negation_insertion_removal"
    ],
    "split": "evaluation"
  },
  {
    "id": "ryeT47FIIS_29",
    "doc": "Momentary fluctuations in attention (perceptual accuracy) correlate with neural activity fluctuations in primate visual areas. Yet, the link between such momentary neural fluctuations and attention state remains to be shown in the human brain. We investigate this link using a real-time cognitive brain machine interface (cBMI) based on steady state visually evoked potentials (SSVEPs): occipital EEG potentials evoked by rhythmically flashing stimuli. Tracking momentary fluctuations in SSVEP power, in real-time, we presented stimuli time-locked to when this power reached (predetermined) high or low thresholds. We observed a significant increase in discrimination accuracy (d') when stimuli were triggered during high (versus low) SSVEP power epochs, at the location cued for attention. Our results indicate a direct link between attention\u2019s effects on perceptual accuracy and and neural gain in EEG-SSVEP power, in the human brain.\n",
    "summary": "With a cognitive brain-machine interface, we indicate a direct link between attentional effects on perceptual accuracy and neural gain in EOG-SSVEP power, in the human brain.",
    "label": 0,
    "original_summary": "With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_og",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "label": 1,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_0",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the second verification that a neural network for perception tasks produces a correct output within a specified tolerance for no input of interest.",
    "label": 0,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_1",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the initial verification that a neural network for perception tasks generates a correct output within a specified tolerance for every input of interest.",
    "label": 1,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_2",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present assurance that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest.",
    "label": 0,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_3",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first attestation that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest.",
    "label": 1,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_4",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first validation that a neural network for perception tasks produces a sound output within a specified tolerance for every input of interest.",
    "label": 1,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_5",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first proof that a neural network for perception tasks produces a correct output within a specific tolerance for every input of interest.",
    "label": 1,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_6",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first verification that a neural network for recognition tasks produces a correct output within a specified tolerance for every input of interest.",
    "label": 1,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_7",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present validation that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest.",
    "label": 0,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_8",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first verification that a neural network for perception tasks produces an incorrect output within a specified tolerance for every input of interest.",
    "label": 0,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_9",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first verification that a neural network for perception tasks produces a correct input within a specified tolerance for every output of interest.",
    "label": 0,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_10",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the second verification that a neural network for perception tasks produces a correct output within a specified tolerance for every other input of interest.",
    "label": 0,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [
      "entity_modification",
      "hallucinated_fact_insertion"
    ],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_11",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first verification that a neural network for perception tasks doesn't produce a correct output within a specified tolerance for every input of interest.",
    "label": 0,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [
      "negation_insertion_removal"
    ],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_12",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first testimony that a neural network for perception tasks produces a proper output within a specified tolerance for every input of interest.",
    "label": 1,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_13",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first verification that an AI system for perception tasks produces a correct output within a specified tolerance for every input of interest.",
    "label": 0,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_14",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first confirmation that a neural network for perception tasks produces a valid output within a specified tolerance for every input of interest.",
    "label": 1,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_15",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first validation that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest.",
    "label": 1,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_16",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first substantiation that a neural network for perception tasks produces an accurate output within a specified tolerance for every input of interest.",
    "label": 1,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_17",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first verification that a neural network for decision tasks produces a correct output within a specified tolerance for every input of interest.",
    "label": 0,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_18",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first verification that a traditional algorithm for perception tasks produces a correct output within a specified tolerance for every input of interest.",
    "label": 0,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_20",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first verification that a neural network for perception tasks produces a correct output within a tolerable range for every input of interest.",
    "label": 1,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_21",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first evidence that a neural network for perception tasks produces a precise output within a specified tolerance for every input of interest.",
    "label": 1,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_22",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first confirmation that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest.",
    "label": 1,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_23",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first verification that a neural network for perception tasks produces a correct input within a specified tolerance for every output of interest.",
    "label": 0,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_24",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first substantiation that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest.",
    "label": 1,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_25",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first verification that a neural network for perception tasks produces a correct output outside of a specified tolerance for every input of interest.",
    "label": 0,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_26",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first verification that a neural network for perception tasks produces an output within a specified tolerance for every input of interest.",
    "label": 0,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_27",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first verification that a decision tree for perception tasks produces a correct output within a specified tolerance for every input of interest.",
    "label": 0,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_29",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first verification that a neural network for recognition tasks produces a correct output within a specified tolerance for every input of interest.",
    "label": 1,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_30",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first confirmation that a neural network for perception tasks generates a correct output within a specified tolerance for every input of interest.",
    "label": 1,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_31",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first demonstration that a neural network for perception tasks produces a reliable output within a specified tolerance for every input of interest.",
    "label": 1,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_32",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present evidence that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest.",
    "label": 0,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_33",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the precursor verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of concern.",
    "label": 0,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_35",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified limit for every input of interest.",
    "label": 1,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_37",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We deny the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest.",
    "label": 0,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "evaluation"
  },
  {
    "id": "B1gtK0NKwr_38",
    "doc": "We present the first verification that a neural network for perception tasks produces\n a correct output within a specified tolerance for every input of interest. We define\n correctness relative to a specification which identifies 1) a state space consisting of\n all relevant states of the world and 2) an observation process that produces neural\n network inputs from the states of the world. Tiling the state and input spaces with\n a finite number of tiles, obtaining ground truth bounds from the state tiles and\n network output bounds from the input tiles, then comparing the ground truth and\n network output bounds delivers an upper bound on the network output error for\n any input of interest. Results from two case studies highlight the ability of our\n technique to deliver tight error bounds for all inputs of interest and show how the\n error bounds vary over the state and input spaces.",
    "summary": "We present the first verification that a neural network for perception tasks produces a correct output within an unspecified tolerance for every input of interest.",
    "label": 0,
    "original_summary": "We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "evaluation"
  },
  {
    "id": "BkxmKgHtwH_og",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "label": 1,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_0",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a framework for evaluating adversarial examples in artificial intelligence and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_1",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a scheme for evaluating adversarial samples in natural language processing and demonstrate that produced adversarial samples are often not semantics-conserving, syntactically correct, or non-suspicious.",
    "label": 1,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_2",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We submit a framework for evaluating adversarial examples in natural language processing and demonstrate that designed adversarial examples are often not semantics-preserving, grammatically intact, or non-suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_3",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a framework for evaluating adversarial examples in natural language processing, but sometimes struggle to demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "antonym_swap",
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_4",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are never semantics-preserving, syntactically correct, or non-suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_5",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often semantics-preserving, syntactically incorrect, or suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_6",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a framework for analyzing adversarial examples in natural language processing and demonstrate that created adversarial examples are often not semantics-preserving, syntactically accurate, or non-suspicious.",
    "label": 1,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_7",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a framework for evaluating adversarial examples in image processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_8",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We display a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, grammatically correct, or non-suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_9",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples often prove to be as semantics-preserving, syntactically correct, or non-suspicious as originally intended.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "hallucinated_fact_insertion",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_10",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that all generated adversarial examples are not semantics-preserving, syntactically correct, or non-suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "entity_modification",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_11",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are always semantics-preserving, syntactically correct, and non-suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_12",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_13",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a methodology for analyzing adversarial examples in natural language processing and illustrate that constructed adversarial examples are often not meaning-retaining, syntactically correct, or non-suspicious.",
    "label": 1,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_15",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We provide a framework for assessing adversarial examples in natural language processing and demonstrate that generated adversarial examples are frequently not semantics-preserving, syntactically real, or non-suspicious.",
    "label": 1,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_16",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We suggest a framework for assessing adversarial examples in natural language processing and demonstrate that constructed adversarial examples are frequently not semantics-preserving, syntactically sound, or non-suspicious.",
    "label": 1,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_17",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We expose a framework for scrutinizing adversarial examples in natural language processing and demonstrate that developed adversarial examples are often not semantics-preserving, grammatically valid, or non-suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_18",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often semantics-changing, syntactically correct, or non-suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_19",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We display a framework for appraising adversarial examples in natural language processing and prove that generated adversarial examples are often not semantics-preserving, grammatically correct, or non-suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_20",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are always not semantics-preserving, syntactically correct, or non-suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_21",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples can be both semantics-preserving, syntactically incorrect, or non-suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_22",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We introduce a framework for assessing adversarial examples in natural language processing, demonstrating that frequently generated adversarial examples are not semantics-preserving, syntactically correct, or non-suspicious.",
    "label": 1,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_23",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a framework for evaluating adversarial examples in natural language processing, but fail to show that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_24",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a framework for evaluating adversarial examples in natural language processing, but we fail to demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "hallucinated_fact_insertion",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_25",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We propose a system for evaluating adversarial examples in natural language processing and demonstrate that produced adversarial examples are often not semantics-preserving, grammatically-precise, or non-suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_26",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a mechanism for assessing adversarial examples in natural language processing and demonstrate that produced adversarial examples are often not sense-preserving, grammatically correct, or non-suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_27",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a framework for evaluating adversarial attacks in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "label": 1,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_28",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present an overview for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "label": 1,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_29",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We present a model for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "label": 1,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "BkxmKgHtwH_30",
    "doc": "Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.",
    "summary": "We bring forward a framework for evaluating adversarial examples in natural language processing and demonstrate that produced adversarial examples are often not semantics-preserving, grammatically legitimate, or non-suspicious.",
    "label": 0,
    "original_summary": "We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "HkglHcSj2N_og",
    "doc": "Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might take a very long time to discover how to reach certain areas of the state-space. In this work we investigate different approaches to incorporate demonstrations to drastically speed up the convergence to a policy able to reach any goal, also surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, our method can be used when only trajectories without expert actions are available, which can leverage kinestetic or third person demonstration.",
    "summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "label": 1,
    "original_summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "HkglHcSj2N_0",
    "doc": "Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might take a very long time to discover how to reach certain areas of the state-space. In this work we investigate different approaches to incorporate demonstrations to drastically speed up the convergence to a policy able to reach any goal, also surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, our method can be used when only trajectories without expert actions are available, which can leverage kinestetic or third person demonstration.",
    "summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Evolutionary algorithms, showing faster convergence than the first and higher final performance than the second.",
    "label": 0,
    "original_summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "HkglHcSj2N_1",
    "doc": "Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might take a very long time to discover how to reach certain areas of the state-space. In this work we investigate different approaches to incorporate demonstrations to drastically speed up the convergence to a policy able to reach any goal, also surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, our method can be used when only trajectories without expert actions are available, which can leverage kinestetic or third person demonstration.",
    "summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Deep Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "label": 0,
    "original_summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "HkglHcSj2N_2",
    "doc": "Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might take a very long time to discover how to reach certain areas of the state-space. In this work we investigate different approaches to incorporate demonstrations to drastically speed up the convergence to a policy able to reach any goal, also surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, our method can be used when only trajectories without expert actions are available, which can leverage kinestetic or third person demonstration.",
    "summary": "We tackle aim-oriented tasks by integrating Hindsight Experience Replay and Imitation Learning algorithms, displaying faster convergence than the first and superior final performance than the second.",
    "label": 1,
    "original_summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "HkglHcSj2N_3",
    "doc": "Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might take a very long time to discover how to reach certain areas of the state-space. In this work we investigate different approaches to incorporate demonstrations to drastically speed up the convergence to a policy able to reach any goal, also surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, our method can be used when only trajectories without expert actions are available, which can leverage kinestetic or third person demonstration.",
    "summary": "We handle goal-focused tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, exemplifying a faster convergence than the first and a higher final performance than the second.",
    "label": 1,
    "original_summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "HkglHcSj2N_4",
    "doc": "Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might take a very long time to discover how to reach certain areas of the state-space. In this work we investigate different approaches to incorporate demonstrations to drastically speed up the convergence to a policy able to reach any goal, also surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, our method can be used when only trajectories without expert actions are available, which can leverage kinestetic or third person demonstration.",
    "summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing slower convergence than the first and higher final performance than the second.",
    "label": 0,
    "original_summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkglHcSj2N_5",
    "doc": "Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might take a very long time to discover how to reach certain areas of the state-space. In this work we investigate different approaches to incorporate demonstrations to drastically speed up the convergence to a policy able to reach any goal, also surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, our method can be used when only trajectories without expert actions are available, which can leverage kinestetic or third person demonstration.",
    "summary": "We tackle aim-driven tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, indicating faster convergence than the former and higher final performance than the latter.",
    "label": 1,
    "original_summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "HkglHcSj2N_6",
    "doc": "Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might take a very long time to discover how to reach certain areas of the state-space. In this work we investigate different approaches to incorporate demonstrations to drastically speed up the convergence to a policy able to reach any goal, also surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, our method can be used when only trajectories without expert actions are available, which can leverage kinestetic or third person demonstration.",
    "summary": "We address goal-oriented tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the former and higher final performance than the latter.",
    "label": 1,
    "original_summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "HkglHcSj2N_7",
    "doc": "Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might take a very long time to discover how to reach certain areas of the state-space. In this work we investigate different approaches to incorporate demonstrations to drastically speed up the convergence to a policy able to reach any goal, also surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, our method can be used when only trajectories without expert actions are available, which can leverage kinestetic or third person demonstration.",
    "summary": "We tackle goal-oriented tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "label": 1,
    "original_summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "HkglHcSj2N_8",
    "doc": "Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might take a very long time to discover how to reach certain areas of the state-space. In this work we investigate different approaches to incorporate demonstrations to drastically speed up the convergence to a policy able to reach any goal, also surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, our method can be used when only trajectories without expert actions are available, which can leverage kinestetic or third person demonstration.",
    "summary": "We tackle irrelevant-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "label": 0,
    "original_summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkglHcSj2N_9",
    "doc": "Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might take a very long time to discover how to reach certain areas of the state-space. In this work we investigate different approaches to incorporate demonstrations to drastically speed up the convergence to a policy able to reach any goal, also surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, our method can be used when only trajectories without expert actions are available, which can leverage kinestetic or third person demonstration.",
    "summary": "We tackle goal-conditioned tasks by combining Foresight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and lower final performance than the second.",
    "label": 0,
    "original_summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "HkglHcSj2N_10",
    "doc": "Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might take a very long time to discover how to reach certain areas of the state-space. In this work we investigate different approaches to incorporate demonstrations to drastically speed up the convergence to a policy able to reach any goal, also surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, our method can be used when only trajectories without expert actions are available, which can leverage kinestetic or third person demonstration.",
    "summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Supervised Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "label": 0,
    "original_summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkglHcSj2N_11",
    "doc": "Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might take a very long time to discover how to reach certain areas of the state-space. In this work we investigate different approaches to incorporate demonstrations to drastically speed up the convergence to a policy able to reach any goal, also surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, our method can be used when only trajectories without expert actions are available, which can leverage kinestetic or third person demonstration.",
    "summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Reinforcement Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "label": 0,
    "original_summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkglHcSj2N_12",
    "doc": "Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might take a very long time to discover how to reach certain areas of the state-space. In this work we investigate different approaches to incorporate demonstrations to drastically speed up the convergence to a policy able to reach any goal, also surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, our method can be used when only trajectories without expert actions are available, which can leverage kinestetic or third person demonstration.",
    "summary": "We approach target-based tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, displaying faster convergence than the first and a greater final performance than the second.",
    "label": 1,
    "original_summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "HkglHcSj2N_13",
    "doc": "Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might take a very long time to discover how to reach certain areas of the state-space. In this work we investigate different approaches to incorporate demonstrations to drastically speed up the convergence to a policy able to reach any goal, also surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, our method can be used when only trajectories without expert actions are available, which can leverage kinestetic or third person demonstration.",
    "summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Transfer Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "label": 0,
    "original_summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "HkglHcSj2N_14",
    "doc": "Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might take a very long time to discover how to reach certain areas of the state-space. In this work we investigate different approaches to incorporate demonstrations to drastically speed up the convergence to a policy able to reach any goal, also surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, our method can be used when only trajectories without expert actions are available, which can leverage kinestetic or third person demonstration.",
    "summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Random Search algorithms, showing faster convergence than the first and higher final performance than the second.",
    "label": 0,
    "original_summary": "We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_og",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "label": 1,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_0",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that removing constant terms from CNN architectures enables interpretability of the denoising method via linear-algebra techniques but hinders generalization performance across noise levels.",
    "label": 0,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_1",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via non-linear techniques and also boosts generalization performance across noise levels.",
    "label": 0,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_2",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that including constant terms in CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "label": 0,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_3",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that discarding constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also improves generalization performance across noise levels.",
    "label": 1,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_4",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that eradicating constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also improves generalization performance across noise levels.",
    "label": 1,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_5",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that suppression of constant terms in CNN architectures demonstrates interpretability of the denoising method via linear-algebra techniques and also enhances generalization performance across noise levels.",
    "label": 1,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_6",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that adding constant terms to CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "label": 0,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_7",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that adding constant terms to CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "label": 0,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_9",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that eliminating unchanging terms from CNN architectures provides interpretability of the denoising method through linear-algebra techniques and also boosts generalization performance across different noise levels.",
    "label": 1,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_10",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that adding constant terms to CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "label": 0,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_11",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that removing constant terms from CNN architectures does not provide interpretability of the denoising method via linear-algebra techniques, but still boosts generalization performance across noise levels.",
    "label": 0,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_12",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that constant terms from CNN architectures sometimes provide interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "label": 0,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_13",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that removing invariable terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also enhances generalization performance across noise levels.",
    "label": 1,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_14",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that getting rid of constant terms from CNN architectures offers interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "label": 1,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_15",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that excluding constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also improves generalization performance across noise levels.",
    "label": 1,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_16",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that keeping constant terms in CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "label": 0,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_17",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that deleting constant terms from CNN architectures increases interpretability of the denoising method via linear-algebra techniques and boosts generalization performance across noise levels.",
    "label": 1,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_18",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques, but doesn't boost generalization performance across noise levels.",
    "label": 0,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_19",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that eliminating constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also enhances generalization performance across noise levels.",
    "label": 1,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_20",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also hinders generalization performance across noise levels.",
    "label": 0,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_21",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that removing static terms from CNN architectures offers interpretability of the denoising method via linear-algebra techniques and also enhances generalization performance across noise levels.",
    "label": 1,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_22",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that exclusion of constant terms from CNN architectures affords interpretability of the denoising method via linear-algebra techniques and also enhances generalization performance across noise levels.",
    "label": 1,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_23",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that eliminating unchanging terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "label": 1,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_24",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that deleting constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also enhances generalization performance across noise levels.",
    "label": 1,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_25",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that deletion of constant terms in CNN architectures showcases interpretability of the denoising method via linear-algebra techniques and also betters generalization performance across noise levels.",
    "label": 1,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_26",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that removing constant terms from CNN architectures provides no significant interpretability of the denoising method via linear-algebra techniques and does not improve generalization performance across noise levels.",
    "label": 0,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_27",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that removing invariable terms from CNN architectures reveals interpretability of the denoising method via linear-algebra techniques and also augments generalization performance across noise levels.",
    "label": 1,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1gOn7h9UH_30",
    "doc": "Deep convolutional networks often append additive constant (\"bias\") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of \"batch normalization\"). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.",
    "summary": "We show that removing constant terms from CNN architectures doesn't provide interpretability of the denoising method via linear-algebra techniques, but still boosts generalization performance across noise levels.",
    "label": 0,
    "original_summary": "We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_og",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "label": 1,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_0",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to infer a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "label": 0,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_2",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a compact state representation with general value functions and on-policy learning is applied to the problem of vision-based steering in piloting airplanes.",
    "label": 0,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_3",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a visual state representation with off-policy temporal difference learning is applied to the problem of vision-based steering in autonomous driving.",
    "label": 0,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_4",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a predictive state representation with general value functions and on-policy learning is applied to the problem of lane keeping in autonomous driving.",
    "label": 0,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_5",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to master a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "label": 1,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_6",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of path planning in autonomous driving.",
    "label": 0,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_7",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a predictive trajectory representation with off-policy temporal difference learning is applied to the problem of vision-based steering in autonomous driving.",
    "label": 0,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_8",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a predictive state representation with general value functions and on-policy learning is applied to the problem of cruise control in autonomous driving.",
    "label": 0,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_9",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a temporal difference representation with general value functions and on-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "label": 0,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_10",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a predictive state representation with general value functions and on-policy learning is applied to the problem of manual-based steering in autonomous driving.",
    "label": 0,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_11",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to understand a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "label": 1,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_12",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a state transition representation with off-policy temporal difference learning is applied to the problem of vision-based steering in autonomous driving.",
    "label": 0,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_13",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the challenge of vision-based steering in autonomous driving.",
    "label": 1,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_14",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn an anticipated state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "label": 1,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_17",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-guided steering in autonomous driving.",
    "label": 1,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_18",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to acquire a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "label": 1,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_19",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a predictive state representation with off-policy temporal difference learning is applied to the problem of manually-controlled steering in autonomous driving.",
    "label": 0,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_20",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of motion planning in autonomous driving.",
    "label": 0,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_21",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the issue of vision-based steering in autonomous driving.",
    "label": 1,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_22",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to obtain a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "label": 1,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_23",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to comprehend a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "label": 1,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_24",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is put into practice to solve the problem of vision-based steering in autonomous driving.",
    "label": 1,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_25",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of visual-based steering in autonomous driving.",
    "label": 1,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_26",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a predictive state representation with off-policy learning is applied to the problem of vision-based steering in manual driving.",
    "label": 0,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_27",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to train a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "label": 0,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_28",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to understand a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "label": 1,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_29",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a predictive state representation with off-policy temporal difference learning is applied to the problem of vision-based parking in autonomous driving.",
    "label": 0,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_30",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a predictive state representation with on-policy learning is applied to the problem of vision-based braking in autonomous driving.",
    "label": 0,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_31",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is used to address the problem of vision-based steering in autonomous driving.",
    "label": 1,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_32",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to discover a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "label": 0,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "B1gi-TVKwB_33",
    "doc": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning that is then used to learn to steer a vehicle with reinforcement learning.   There are three components being learned simultaneously:  (1) the off-policy predictions as a compact representation of state, (2) the behavior policy distribution for estimating the off-policy predictions, and (3) the deterministic policy gradient for learning to act.   A behavior policy discriminator is learned and used for estimating the important sampling ratios needed to learn the predictive representation off-policy with general value functions (GVFs).   A linear deterministic policy gradient method is used to train the agent with only the predictive representations while the predictions are being learned.   All three components are combined, demonstrated and evaluated on the problem of steering the vehicle from images in the TORCS racing simulator environment.\n Steering from only images is a challenging problem where evaluation is completed on a held-out set of tracks that were never seen during training in order to measure the generalization of the predictions and controller.   Experiments show the proposed method is able to steer smoothly and navigate many but not all of the tracks available in TORCS with performance that exceeds DDPG using only images as input and approaches the performance of an ideal non-vision based kinematics model.",
    "summary": "An algorithm to learn a predictive state representation with off-policy learning is applied to the problem of image-based braking in autonomous driving.",
    "label": 0,
    "original_summary": "An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_og",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "label": 1,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_0",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is abundant and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_1",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of new GANs when training data is limited and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_2",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and identical.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_3",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a novel, simple, and efficient data augmentation method that impairs the performances of existing GANs when training data is limited and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_4",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a novel, simple, and efficient data augmentation method that hinders the performances of existing GANs when training data is limited and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_5",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a new, simple, and efficient data augmentation method that improves the performances of existing GANs when training data is limited and diverse.",
    "label": 1,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_6",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a novel, simple, and inefficient data augmentation method that lowers the performances of existing GANs when training data is limited and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_7",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is unlimited but diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_9",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a convenient, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_10",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a novel, complex, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_11",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is significant and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_12",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is scarce and narrow.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_13",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a complex, ineffective, and time-consuming data augmentation method that lowers the performances of existing GANs when training data is limited and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_14",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a unique, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_15",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a novel, complicated, and inefficient data augmentation method that boosts the performances of existing GANs when training data is abundant and basic.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_16",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is unlimited and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_17",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a robust, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_18",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when testing data is limited and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_19",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a reliable, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_20",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is unlimited and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_22",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a complex, slow, and inefficient data augmentation method that reduces the performances of existing GANs when training data is limited and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_23",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing CNNs when training data is limited and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_24",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a scalable, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_25",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a powerful, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_26",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a novel, simple, and inefficient data augmentation method that boosts the performances of existing GANs when training data is limited and uniform.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_27",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a novel, complex, and inefficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_28",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a practical, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "HkxHFj5BdV_29",
    "doc": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review.",
    "summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited but not diverse.",
    "label": 0,
    "original_summary": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ",
    "edit_types": [
      "hallucinated_fact_insertion",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_og",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "label": 1,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_1",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end fashion.",
    "label": 1,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_2",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a new graph inference learning framework by building structure relations to deduce unknown node labels from the existing labeled nodes in an end-to-end way.",
    "label": 1,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_3",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by building structure relations to infer unknown edge labels from those labeled nodes in an end-to-end way.",
    "label": 0,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_4",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose an inefficient graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "label": 0,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_5",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a new graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in a semi-supervised way.",
    "label": 0,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_6",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a new graph inference learning framework by building structure relations to infer unknown node labels and node attributes from those labeled nodes in an end-to-end way.",
    "label": 0,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [
      "entity_modification",
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_7",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by developing structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "label": 1,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_8",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled edges in an end-to-end way.",
    "label": 0,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_9",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a powerful graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "label": 0,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_10",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose an innovative graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "label": 1,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_11",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by constructing structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "label": 1,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_12",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by establishing connectivity relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "label": 1,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_13",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by building structure relations to infer unknown node labels and graph structure from those labeled nodes in an end-to-end way.",
    "label": 0,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_14",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by forming structural relations to predict unknown node labels from labeled nodes in an end-to-end way.",
    "label": 0,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_15",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel gradient-based inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "label": 0,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_17",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by building structure relations to infer known node labels from those labeled nodes in an end-to-end way.",
    "label": 0,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_18",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by building structure relations to infer unknown feature vectors from those labeled nodes in an end-to-end way.",
    "label": 0,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_19",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a unique graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "label": 0,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_20",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by building structure bonds to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "label": 1,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_21",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by building connectivity relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "label": 1,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_22",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by building structure relations to infer unknown attributes from those labeled nodes in an end-to-end way.",
    "label": 0,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_23",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled clusters in an end-to-end way.",
    "label": 0,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_24",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by building structure relations to infer known node labels from those labeled nodes in a semi-supervised way.",
    "label": 0,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_25",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by building structure relations to deduce unknown node labels from those labeled nodes in an end-to-end way.",
    "label": 1,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_26",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by building structure connections to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "label": 1,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_27",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those unlabeled nodes in an end-to-end way.",
    "label": 0,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_28",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by creating structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "label": 1,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_29",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by establishing structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "label": 1,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_30",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by building structure relations to infer unknown node attributes from those labeled nodes in an end-to-end way.",
    "label": 0,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_32",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by using random forest to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "label": 0,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "r1evOhEKvH_33",
    "doc": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
    "summary": "We propose a novel graph inference learning framework by building structure relations to infer unknown link labels from those labeled nodes in an end-to-end way.",
    "label": 0,
    "original_summary": " We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_og",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "label": 1,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_0",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts the uncertainties of features in inverted layers of DNNs using a reparameterization trick and combines them for detecting OOD samples when solving classification tasks.",
    "label": 0,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [
      "entity_modification",
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_1",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts the certainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "label": 0,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_2",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts the uncertainties of distribution in each layer of DNNs using a reparameterization trick and combines them for detecting OOD samples when solving classification tasks.",
    "label": 0,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [
      "entity_modification",
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_3",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that disables features' uncertainties in each layer of DNNs and integrates them for detecting OOD samples when solving classification tasks.",
    "label": 1,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_4",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts the uncertainties of attributes in each layer of DNNs and combines them for detecting OOD samples when accomplishing classification tasks.",
    "label": 1,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_6",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs using a parameterization trick and combines them for detecting OOD samples when solving classification tasks.",
    "label": 0,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_7",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a solution that extracts the uncertainties of features in each layer of DNNs and combines them to identify OOD samples when tackling classification tasks.",
    "label": 1,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_8",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a technique that extracts the uncertainties of features in each layer of DNNs and amalgamates them for detecting OOD samples when resolving classification tasks.",
    "label": 1,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_9",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts the uncertainties of features in the output layer of DNNs using a reparameterization trick and combines them for detecting OOD samples when solving classification tasks.",
    "label": 0,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [
      "entity_modification",
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_10",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs using a simple algorithm and combines them for detecting OOD samples when solving classification tasks.",
    "label": 0,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_11",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that ignores the uncertainties of features in each layer of DNNs for detecting OOD samples when solving classification tasks.",
    "label": 0,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_12",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them for detecting OOD samples when solving clustering tasks.",
    "label": 0,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [
      "entity_modification",
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_13",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs using a hyperparameter optimization and combines them for detecting OOD samples when solving classification tasks.",
    "label": 0,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_14",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts the uncertainties of features in each layer of CNNs using a reparameterization trick and combines them for detecting OOD samples when solving regression tasks.",
    "label": 0,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_15",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a methodology that extracts the uncertainties of features in each layer of Deep Neural Networks and combines them for detecting OOD samples when resolving classification tasks.",
    "label": 1,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_16",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts the uncertainties of features in each output layer of DNNs using a reparameterization trick and combines them for detecting OOD samples when solving classification tasks.",
    "label": 1,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_17",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts the uncertainties of inputs in each layer of DNNs using a reparameterization trick and combines them for detecting OOD samples when solving classification tasks.",
    "label": 1,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_18",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a procedure that extracts the uncertainties of features in each layer of DNNs and blends them to detect OOD samples when solving classification tasks.",
    "label": 1,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_19",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and ignores them when detecting OOD samples when solving classification tasks.",
    "label": 0,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [
      "antonym_swap",
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_20",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a procedure that extracts the uncertainties of features in each layer of DNNs and aggregates them to distinguish OOD samples when solving classification tasks.",
    "label": 1,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_21",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts the uncertainties of features in each layer of CNNs and combines them for detecting OOD samples when solving regression tasks.",
    "label": 0,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_22",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts the uncertainties of features in no layers of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "label": 0,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_23",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a strategy that extracts the uncertainties of features in each layer of DNNs and amalgamates them for detecting OOD samples when solving classification tasks.",
    "label": 1,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_24",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts the uncertainties of samples in each layer of DNNs using a reparameterization trick and combines them for detecting OOD samples when solving classification tasks.",
    "label": 0,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [
      "entity_modification",
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_25",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs but does not combine them for detecting OOD samples when solving classification tasks.",
    "label": 0,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_27",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts not uncertainties, but the features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "label": 0,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "rklVOnNtwH_28",
    "doc": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
    "summary": "We propose a method that extracts the certainties of features in each layer of DNNs using a reparameterization trick and combines them for detecting OOD samples when solving classification tasks.",
    "label": 0,
    "original_summary": "We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.",
    "edit_types": [
      "antonym_swap",
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "SJxfxnA9K7_og",
    "doc": "We propose a novel method for incorporating conditional information into a generative adversarial network (GAN) for structured prediction tasks. This method is based on fusing features from the generated and conditional information in feature space and allows the discriminator to better capture higher-order statistics from the data. This method also increases the strength of the signals passed through the network where the real or generated data and the conditional data agree. The proposed method is conceptually simpler than the joint convolutional neural network - conditional Markov random field (CNN-CRF) models and enforces higher-order consistency without being limited to a very specific class of high-order potentials. Experimental results demonstrate that this method leads to improvement on a variety of different structured prediction tasks including image synthesis, semantic segmentation, and depth estimation.",
    "summary": "We propose a novel way to incorporate conditional image information into the discriminator of GANs using feature fusion that can be used for structured prediction tasks.",
    "label": 1,
    "original_summary": "We propose a novel way to incorporate conditional image information into the discriminator of GANs using feature fusion that can be used for structured prediction tasks.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "SJxfxnA9K7_0",
    "doc": "We propose a novel method for incorporating conditional information into a generative adversarial network (GAN) for structured prediction tasks. This method is based on fusing features from the generated and conditional information in feature space and allows the discriminator to better capture higher-order statistics from the data. This method also increases the strength of the signals passed through the network where the real or generated data and the conditional data agree. The proposed method is conceptually simpler than the joint convolutional neural network - conditional Markov random field (CNN-CRF) models and enforces higher-order consistency without being limited to a very specific class of high-order potentials. Experimental results demonstrate that this method leads to improvement on a variety of different structured prediction tasks including image synthesis, semantic segmentation, and depth estimation.",
    "summary": "We introduce a novel approach to incorporating conditional image information into the discriminator of GANs using feature fusion, which can be used for structured prediction tasks.",
    "label": 0,
    "original_summary": "We propose a novel way to incorporate conditional image information into the discriminator of GANs using feature fusion that can be used for structured prediction tasks.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "SJxfxnA9K7_1",
    "doc": "We propose a novel method for incorporating conditional information into a generative adversarial network (GAN) for structured prediction tasks. This method is based on fusing features from the generated and conditional information in feature space and allows the discriminator to better capture higher-order statistics from the data. This method also increases the strength of the signals passed through the network where the real or generated data and the conditional data agree. The proposed method is conceptually simpler than the joint convolutional neural network - conditional Markov random field (CNN-CRF) models and enforces higher-order consistency without being limited to a very specific class of high-order potentials. Experimental results demonstrate that this method leads to improvement on a variety of different structured prediction tasks including image synthesis, semantic segmentation, and depth estimation.",
    "summary": "We propose an innovative way to incorporate conditional image information into the discriminator of GANs using feature fusion that can be used for structured prediction tasks.",
    "label": 1,
    "original_summary": "We propose a novel way to incorporate conditional image information into the discriminator of GANs using feature fusion that can be used for structured prediction tasks.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "SJxfxnA9K7_3",
    "doc": "We propose a novel method for incorporating conditional information into a generative adversarial network (GAN) for structured prediction tasks. This method is based on fusing features from the generated and conditional information in feature space and allows the discriminator to better capture higher-order statistics from the data. This method also increases the strength of the signals passed through the network where the real or generated data and the conditional data agree. The proposed method is conceptually simpler than the joint convolutional neural network - conditional Markov random field (CNN-CRF) models and enforces higher-order consistency without being limited to a very specific class of high-order potentials. Experimental results demonstrate that this method leads to improvement on a variety of different structured prediction tasks including image synthesis, semantic segmentation, and depth estimation.",
    "summary": "We introduce a novel technique to incorporate conditional image information into the generator of GANs using feature space fusion that can be used for unstructured prediction tasks.",
    "label": 0,
    "original_summary": "We propose a novel way to incorporate conditional image information into the discriminator of GANs using feature fusion that can be used for structured prediction tasks.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "SJxfxnA9K7_4",
    "doc": "We propose a novel method for incorporating conditional information into a generative adversarial network (GAN) for structured prediction tasks. This method is based on fusing features from the generated and conditional information in feature space and allows the discriminator to better capture higher-order statistics from the data. This method also increases the strength of the signals passed through the network where the real or generated data and the conditional data agree. The proposed method is conceptually simpler than the joint convolutional neural network - conditional Markov random field (CNN-CRF) models and enforces higher-order consistency without being limited to a very specific class of high-order potentials. Experimental results demonstrate that this method leads to improvement on a variety of different structured prediction tasks including image synthesis, semantic segmentation, and depth estimation.",
    "summary": "We suggest a novel way of removing conditional image information from discriminator of GANs using feature fusion that can be used for structured prediction tasks.",
    "label": 0,
    "original_summary": "We propose a novel way to incorporate conditional image information into the discriminator of GANs using feature fusion that can be used for structured prediction tasks.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "SJxfxnA9K7_5",
    "doc": "We propose a novel method for incorporating conditional information into a generative adversarial network (GAN) for structured prediction tasks. This method is based on fusing features from the generated and conditional information in feature space and allows the discriminator to better capture higher-order statistics from the data. This method also increases the strength of the signals passed through the network where the real or generated data and the conditional data agree. The proposed method is conceptually simpler than the joint convolutional neural network - conditional Markov random field (CNN-CRF) models and enforces higher-order consistency without being limited to a very specific class of high-order potentials. Experimental results demonstrate that this method leads to improvement on a variety of different structured prediction tasks including image synthesis, semantic segmentation, and depth estimation.",
    "summary": "We suggest a novel way to incorporate image information into the discriminator of GANs using feature fusion that can't be used for structured prediction tasks.",
    "label": 0,
    "original_summary": "We propose a novel way to incorporate conditional image information into the discriminator of GANs using feature fusion that can be used for structured prediction tasks.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "SJxfxnA9K7_8",
    "doc": "We propose a novel method for incorporating conditional information into a generative adversarial network (GAN) for structured prediction tasks. This method is based on fusing features from the generated and conditional information in feature space and allows the discriminator to better capture higher-order statistics from the data. This method also increases the strength of the signals passed through the network where the real or generated data and the conditional data agree. The proposed method is conceptually simpler than the joint convolutional neural network - conditional Markov random field (CNN-CRF) models and enforces higher-order consistency without being limited to a very specific class of high-order potentials. Experimental results demonstrate that this method leads to improvement on a variety of different structured prediction tasks including image synthesis, semantic segmentation, and depth estimation.",
    "summary": "We propose a novel approach to incorporate conditional image information into the discriminator of GANs using feature fusion that can be used for structured prediction tasks.",
    "label": 1,
    "original_summary": "We propose a novel way to incorporate conditional image information into the discriminator of GANs using feature fusion that can be used for structured prediction tasks.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "SJxfxnA9K7_9",
    "doc": "We propose a novel method for incorporating conditional information into a generative adversarial network (GAN) for structured prediction tasks. This method is based on fusing features from the generated and conditional information in feature space and allows the discriminator to better capture higher-order statistics from the data. This method also increases the strength of the signals passed through the network where the real or generated data and the conditional data agree. The proposed method is conceptually simpler than the joint convolutional neural network - conditional Markov random field (CNN-CRF) models and enforces higher-order consistency without being limited to a very specific class of high-order potentials. Experimental results demonstrate that this method leads to improvement on a variety of different structured prediction tasks including image synthesis, semantic segmentation, and depth estimation.",
    "summary": "We present a unique way to incorporate conditional image information into the discriminator of GANs using feature fusion that may not be used for structured prediction tasks.",
    "label": 0,
    "original_summary": "We propose a novel way to incorporate conditional image information into the discriminator of GANs using feature fusion that can be used for structured prediction tasks.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "SJxfxnA9K7_13",
    "doc": "We propose a novel method for incorporating conditional information into a generative adversarial network (GAN) for structured prediction tasks. This method is based on fusing features from the generated and conditional information in feature space and allows the discriminator to better capture higher-order statistics from the data. This method also increases the strength of the signals passed through the network where the real or generated data and the conditional data agree. The proposed method is conceptually simpler than the joint convolutional neural network - conditional Markov random field (CNN-CRF) models and enforces higher-order consistency without being limited to a very specific class of high-order potentials. Experimental results demonstrate that this method leads to improvement on a variety of different structured prediction tasks including image synthesis, semantic segmentation, and depth estimation.",
    "summary": "We propose a groundbreaking way to incorporate conditional image information into the discriminator of GANs using feature fusion that can be used for structured prediction tasks.",
    "label": 1,
    "original_summary": "We propose a novel way to incorporate conditional image information into the discriminator of GANs using feature fusion that can be used for structured prediction tasks.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "SJxfxnA9K7_14",
    "doc": "We propose a novel method for incorporating conditional information into a generative adversarial network (GAN) for structured prediction tasks. This method is based on fusing features from the generated and conditional information in feature space and allows the discriminator to better capture higher-order statistics from the data. This method also increases the strength of the signals passed through the network where the real or generated data and the conditional data agree. The proposed method is conceptually simpler than the joint convolutional neural network - conditional Markov random field (CNN-CRF) models and enforces higher-order consistency without being limited to a very specific class of high-order potentials. Experimental results demonstrate that this method leads to improvement on a variety of different structured prediction tasks including image synthesis, semantic segmentation, and depth estimation.",
    "summary": "We propose a novel way to incorporate non-conditional image information into the discriminator of GANs using feature fusion that can be used for structured prediction tasks.",
    "label": 0,
    "original_summary": "We propose a novel way to incorporate conditional image information into the discriminator of GANs using feature fusion that can be used for structured prediction tasks.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "SJxfxnA9K7_15",
    "doc": "We propose a novel method for incorporating conditional information into a generative adversarial network (GAN) for structured prediction tasks. This method is based on fusing features from the generated and conditional information in feature space and allows the discriminator to better capture higher-order statistics from the data. This method also increases the strength of the signals passed through the network where the real or generated data and the conditional data agree. The proposed method is conceptually simpler than the joint convolutional neural network - conditional Markov random field (CNN-CRF) models and enforces higher-order consistency without being limited to a very specific class of high-order potentials. Experimental results demonstrate that this method leads to improvement on a variety of different structured prediction tasks including image synthesis, semantic segmentation, and depth estimation.",
    "summary": "We propose a new way of integrating conditional image information into the discriminator of GANs using feature fusion that can be applied for structured prediction tasks.",
    "label": 1,
    "original_summary": "We propose a novel way to incorporate conditional image information into the discriminator of GANs using feature fusion that can be used for structured prediction tasks.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_og",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "label": 1,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_0",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Wisdom-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_1",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning unsuitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_2",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning unsuitable contracts to themselves.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_3",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-naive Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate altruistic workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_4",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Cognition-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_5",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to demotivate self-interested workers to achieve suboptimal collaboration by assigning unsuitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_6",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Thought-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_7",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to someone else.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_8",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Reason-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_9",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to demotivate self-interested workers to perform poorly by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_10",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Unmindful Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate selfless workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_11",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a robot to motivationally abuse self-interested workers to achieve optimal manipulation by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_12",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Brain-conscious Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_13",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-naive Multi-agent Management Reinforcement Learning (M^3RL) for training a worker to motivate self-interested managers to achieve optimal collaboration by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_14",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate selfish workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_15",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Psyche-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_16",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a worker to demotivate self-interested managers to achieve optimal collaboration by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_17",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to collaborate sub-optimally by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_18",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a student to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_19",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to manipulate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_20",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to demotivate self-interested workers to achieve optimal collaboration by assigning unsuitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_21",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve suboptimal collaboration by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_22",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Sentience-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_23",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to demotivate self-interested workers to achieve suboptimal collaboration by assigning unsuitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_24",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning unsuitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_25",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Intelligence-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_26",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Cognitive-conscious Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_27",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve suboptimal collaboration by assigning unsuitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_28",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mindful Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "BkzeUiRcY7_29",
    "doc": "Most of the prior work on multi-agent reinforcement learning (MARL) achieves optimal collaboration by directly learning a policy for each agent to maximize a common reward. In this paper, we aim to address this from a different angle. In particular, we consider scenarios where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. For achieving optimal coordination among these agents, we train a super agent (i.e., the manager) to manage them by first inferring their minds based on both current and past observations and then initiating contracts to assign suitable tasks to workers and promise to reward them with corresponding bonuses so that they will agree to work together. The objective of the manager is to maximize the overall productivity as well as minimize payments made to the workers for ad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL), which consists of agent modeling and policy learning. We have evaluated our approach in two environments, Resource Collection and Crafting, to simulate multi-agent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of our approach in modeling worker agents' minds online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation.",
    "summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal competition by assigning suitable contracts to them.",
    "label": 0,
    "original_summary": "We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_og",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "label": 1,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_1",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We study the lack of impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "label": 0,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_2",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We evaluate the impact of using unique subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "label": 0,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_3",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We evaluate the impact of using various subword units types on the quality of the resulting representations when implemented to model syntax, semantics, and morphology.",
    "label": 1,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_4",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We analyze the effect of using alternate subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "label": 0,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_5",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We study the impact of using different kinds of units on the quality of the resulting representations when used to model only syntax and semantics, but not morphology.",
    "label": 0,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_6",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We analyze the effects of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "label": 1,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_7",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We study the impact of using different kinds of subword units on the lack of quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "label": 0,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_8",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We study the impact of using diverse subword units categories on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "label": 1,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_9",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We study the effect of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology, but the results were inconclusive.",
    "label": 0,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [
      "antonym_swap",
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_10",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We research the influence of using different sorts of subword units on the quality of the produced representations when used to model syntax, semantics, and morphology.",
    "label": 1,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_11",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, but not morphology and semantics.",
    "label": 0,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [
      "entity_modification",
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_12",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model semantics, but not syntax and morphology.",
    "label": 0,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_13",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We study the impact of using different kinds of units on the quality of the resulting representations when used to model only semantics and morphology, but not syntax.",
    "label": 0,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [
      "hallucinated_fact_insertion",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_14",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We research the impact of employing different categories of subword units on the quality of the produced representations when utilized to model syntax, semantics, and morphology.",
    "label": 1,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_15",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations only when used to model syntax, and not semantics or morphology.",
    "label": 0,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_16",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We scrutinize the impact of employing various subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "label": 1,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_17",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We study the influence of using different subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "label": 1,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_18",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We study the positive impact of using different kinds of units on the quality of the resulting representations when used to model only semantics, and not syntax or morphology.",
    "label": 0,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [
      "antonym_swap",
      "hallucinated_fact_insertion",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_19",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We study the limited impact of using different kinds of subword units on the quality of the resulting representations when used to model only syntax and morphology, but not semantics.",
    "label": 0,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [
      "antonym_swap",
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_21",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We explore the effect of using different types of subword elements on the quality of the resulting representations when used to represent syntax, semantics, and morphology.",
    "label": 1,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_22",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We study the impact of using different kinds of units on the quality of the resulting representations when used to model morphology, but not syntax and semantics.",
    "label": 0,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [
      "hallucinated_fact_insertion",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_23",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We examine the impact of utilizing various subword units on the quality of the resulting representations used to model syntax, semantics, and morphology.",
    "label": 1,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_24",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We study the impact of using different kinds of character units on the quality of the resulting representations when used to model syntax and morphology, but not semantics.",
    "label": 0,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [
      "entity_modification",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_25",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We study the impact of using different kinds of character units on the quality of the resulting representations when used to model syntax, but not morphology and semantics.",
    "label": 0,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [
      "entity_modification",
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_27",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We analyze the impact of using dissimilar kinds of subword units on the quality of the resulting representations when implemented to model syntax, semantics, and morphology.",
    "label": 1,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_28",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We study the impact of using different kinds of character units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "label": 0,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_30",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We study the impact of using different kinds of character units on the quality of the resulting representations when used to model semantics, but not syntax and morphology.",
    "label": 0,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [
      "entity_modification",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "B1x0E2C5tQ_31",
    "doc": "Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.   We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.",
    "summary": "We study the negative impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "label": 0,
    "original_summary": "We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_og",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "label": 1,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_1",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for image modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_2",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for text modeling without conditioning the decoder on latent variables, which improves the quality of text generation and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "entity_modification",
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_3",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which worsens the quality of text generation and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_4",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a framework of variational autoencoders for text modeling without weakening the decoder, which enhances the quality of text generation and interpretability of acquired representations.",
    "label": 1,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_6",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of generative adversarial networks for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_7",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for text modeling with a unimodal prior distribution, which improves the quality of text generation and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "entity_modification",
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_8",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for text modelling without curbing the decoder, which improves text generation quality and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_9",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of generative adversarial networks for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_10",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, ushering in enhanced quality of text generation and interpretability of acquired representations.",
    "label": 1,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_11",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for text modeling without weakening the encoder, which improves the quality of text generation and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_12",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for text modeling with a unimodal prior distribution, which improves the quality of text generation and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_13",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for text modeling without diluting the decoder, which enhances the quality of text generation and interpretability of acquired representations.",
    "label": 1,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_16",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for text summarization without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_18",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose an approach of variational autoencoders for text modeling without reducing the decoder's efficiency, which enhances the quality of text generation and interpretability of acquired representations.",
    "label": 1,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_20",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for text modeling without weakening the encoder, which improves the quality of text generation and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_23",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for text modelling without reducing the decoder, which boosts the quality of text generation and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_25",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for text modeling without a multimodal prior distribution, which improves the quality of text generation and interpretation of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "entity_modification",
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_28",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for text modeling without strengthening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_29",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of convolutional neural networks and VAEs for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_30",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for image modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_31",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a modified model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_32",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for image modeling without weakening the capacity of the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_33",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for text modelling without weakening the decoder, which amplifies text generation quality and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_34",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of convolutional neural networks for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_35",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, thereby bolstering the quality of text generation and interpretability of acquired representations.",
    "label": 1,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_36",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for text modeling without multi-task learning, which improves the quality of text generation and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "entity_modification",
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "H1eZ6sRcFm_37",
    "doc": "Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.",
    "summary": "We propose a model of variational autoencoders for image modeling without weakening the encoder, which improves the quality of text generation and interpretability of acquired representations.",
    "label": 0,
    "original_summary": "We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_og",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "label": 1,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_0",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We gauge the energy expense in terms of money (cloud credits) and carbon footprint of developing recently accomplished neural network models for NLP. Costs are high.",
    "label": 1,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_2",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We gauge the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "label": 1,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_4",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We quantify the energy tariffs in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "label": 1,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_5",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We appraise the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "label": 1,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_6",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We quantify the energy cost in terms of money spent on expensive hardware and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "label": 0,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [
      "entity_modification",
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_7",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We evaluate the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "label": 1,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_8",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We quantify the energy savings in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are low.",
    "label": 0,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_9",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently unsuccessful neural network models for NLP. Costs are low.",
    "label": 0,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_10",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of building successful neural network models for NLP. Costs are high.",
    "label": 0,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_11",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We measure the cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "label": 1,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_12",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We assess the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "label": 1,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_14",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of developing recently effective neural network models for NLP. Costs are high.",
    "label": 1,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_15",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We measure the energy expense in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "label": 1,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_16",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We quantify the financial cost in terms of time (cloud credits) and carbon footprint of training recent neural network models for NLP. Costs are moderate.",
    "label": 0,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_17",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We compute the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "label": 1,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_18",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We assess the energy outlay in terms of money (cloud credits) and carbon footprint of training lately successful neural network models for NLP. Costs are high.",
    "label": 1,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_21",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We compute the energy expenditure in terms of money (cloud credits) and carbon footprint of training recent high-performing neural network models for NLP. Costs are high.",
    "label": 1,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_22",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We quantify the energy cost in terms of money saved (cloud credits) and carbon footprint from training recently successful neural network models for NLP. Costs are low.",
    "label": 0,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_23",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We underestimate the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are not high.",
    "label": 0,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_24",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We quantify the energy expense in relation to money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "label": 0,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_25",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We assess the energy outlay in terms of money (cloud credits) and carbon footprint of developing recently productive neural network models for NLP. Costs are high.",
    "label": 1,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_26",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We measure the energy consumption in terms of money (cloud credits) and carbon emissions of training recently successful neural network models for NLP. Costs are high.",
    "label": 1,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_27",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful economic neural network models for NLP. Costs are high.",
    "label": 0,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [
      "antonym_swap",
      "hallucinated_fact_insertion"
    ],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_28",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We quantify the financial cost in terms of money (cloud credits) and carbon footprint of training recently unsuccessful neural network models for NLP. Costs are high.",
    "label": 0,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_29",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We misconstrue the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are low.",
    "label": 0,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "rJg6Zh5Xer_30",
    "doc": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
    "summary": "We exaggerate the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are insurmountable.",
    "label": 0,
    "original_summary": "We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_og",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "label": 1,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_0",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount post-stimulus baseline activity, which improves decoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_1",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to inefficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_2",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to inefficiently discount pre-stimulus baseline activity, which improves decoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_3",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-by-time matrix decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_4",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently disregard pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "label": 1,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_5",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-by-time tensor decomposition based on negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_6",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-by-time tensor decomposition predicated on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "label": 1,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_7",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount post-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_8",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We reduced single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity, which improves decoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_9",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently preserve pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_10",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-by-time tensor decomposition founded on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "label": 1,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_11",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity, which improves encoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_12",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-through-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_14",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial time-by-space tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity, which improves decoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_15",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount post-stimulus baseline activity that impairs decoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_18",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-by-time decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity, which improves decoding performance on data with negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_19",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended multi-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity, which improves decoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_20",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to inefficiently discount pre-stimulus baseline activity that impairs decoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_21",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-by-time tensor decomposition centered on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "label": 1,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_22",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-by-time tensor decomposition relying on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "label": 1,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_23",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-by-time tensor decomposition based on negative matrix factorization to efficiently discount pre-stimulus baseline activity, which improves decoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_24",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity, which worsens decoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "antonym_swap",
      "negation_insertion_removal"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_25",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial spatial-by-time tensor decomposition based on non-negative matrix factorization to effectively discount pre-stimulus baseline activity that enhances decoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_26",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-by-time tensor decomposition based on non-positive matrix factorization to efficiently discount pre-stimulus baseline activity, which improves decoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "antonym_swap",
      "entity_modification"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_27",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves encoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_28",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We shortened single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "antonym_swap"
    ],
    "split": "test"
  },
  {
    "id": "Bki1Ct1AW_29",
    "doc": "Activity of populations of sensory neurons carries stimulus information in both the temporal and the spatial dimensions. This poses the question of how to compactly represent all the information that the population codes carry across all these dimensions. Here, we developed an analytical method to factorize a large number of retinal ganglion cells' spike trains into a robust low-dimensional representation that captures efficiently both their spatial and temporal information. In particular, we extended previously used single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity. On data recorded from retinal ganglion cells with strong pre-stimulus baseline, we showed that in situations were the stimulus elicits a strong change in firing rate, our extensions yield a boost in stimulus decoding performance. Our results thus suggest that taking into account the baseline can be important for finding a compact information-rich representation of neural activity.",
    "summary": "We extended a double-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "label": 0,
    "original_summary": "We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.",
    "edit_types": [
      "entity_modification"
    ],
    "split": "test"
  }
]